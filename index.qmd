---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
format:
  html: default
  pdf: default
execute:
  echo: False
  eval: true
---

# üå≥ Decision Tree Challenge - Feature Importance and Variable Encoding

## The Decision Tree Problem üéØ

Decision trees can be helpful when searching for analysis that has interpretability and versitility. However, flaws occur when we mark categorical variables as numbers. Decision trees react as if the sequence of numbers have a meaning, which can jumble the transparency of our analysis. This is why feature importance is established. Feature importance measures the amount that each variable contributes to the improvement of prediction accuracy. This applies for all splits in the tree. This is the path to understanding which variables have the greatest significance as predictors.

## Discussion Questions for Challenge

**Narrative**

 1. 
  Zip codes are not averaged, added, or used in any arithmetic sequences. They should be modeled categorically since they refer to a town, and not a number of some kind. In the sense of real estate, each zip code should be categorical, which allows them to be intercepts. Zip codes capture areas and represent the housing prices in that location. It will influence price analysis in a categorical way.

2. 
  R has more sensible splits because of packages like randomforest. This helps to pass variables directly in groupings that are produced by the algorithm itself. In contrast, Python uses packages like scikit-learn that requires you to encode categorical variables. If you do not speficfy beforehand, the package assumes all inputs are numeric. The rpart "splits a numeric matrix describing the splits: ‚Ä¶ the row label is the name of the split variable ‚Ä¶ For a factor ‚Ä¶ the index column contains the row number of the csplit matrix.‚Äù (Therneau 2025) R give the freedom to the algorithm to group ZIP codes as categorical values based on outcome distribution. Having to do a one-hot encode will create a dilution of feature importance, since it will be spread into many dummy columns.


